{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4d138f",
   "metadata": {},
   "source": [
    "## Part 1: Hugging Face Datasets\n",
    "\n",
    "### 1.1 Loading Pre-existing Datasets\n",
    "\n",
    "The Datasets library makes it easy to load and process data for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0348ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\valer\\Mega\\Programming\\SemanticWeb\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|██████████| 67349/67349 [00:00<00:00, 1804997.96 examples/s]\n",
      "Generating validation split: 100%|██████████| 872/872 [00:00<00:00, 220353.84 examples/s]\n",
      "Generating test split: 100%|██████████| 1821/1821 [00:00<00:00, 466574.68 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Number of examples: 100\n",
      "Features: {'sentence': Value('string'), 'label': ClassLabel(names=['negative', 'positive']), 'idx': Value('int32')}\n",
      "\n",
      "First example:\n",
      "{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a sample text dataset (simulating graph problem descriptions)\n",
    "# For demonstration, we'll use a small dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"train[:100]\")\n",
    "\n",
    "print(f\"Dataset type: {type(dataset)}\")\n",
    "print(f\"Number of examples: {len(dataset)}\")\n",
    "print(f\"Features: {dataset.features}\")\n",
    "\n",
    "# View first example\n",
    "print(\"\\nFirst example:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77c3b81",
   "metadata": {},
   "source": [
    "### 1.2 Custom Datasets\n",
    "\n",
    "For graph problems, we'll need to use our own datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "190e7778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'answer', 'task'],\n",
      "        num_rows: 4821\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query', 'answer', 'task'],\n",
      "        num_rows: 961\n",
      "    })\n",
      "})\n",
      "\n",
      "--- First Example Record (train[0]) ---\n",
      "  query: Determine if there is a path between two nodes in the graph. Note that (i,j) means that node i and node j are connected with an undirected edge.\n",
      "Graph: (0,12) (0,13) (0,2) (0,14) (0,23) (0,8) (0,1) (0...\n",
      "  answer: The answer is yes.\n",
      "  task: connectivity\n"
     ]
    }
   ],
   "source": [
    "from nlgraph_loader import load_nlgraph\n",
    "\n",
    "dataset = load_nlgraph()\n",
    "print(dataset)\n",
    "print(\"\\n--- First Example Record (train[0]) ---\")\n",
    "example = dataset['train'][0]\n",
    "for key, value in example.items():\n",
    "    # Truncate long values for readability\n",
    "    value_str = str(value)\n",
    "    if len(value_str) > 200:\n",
    "        value_str = value_str[:200] + \"...\"\n",
    "    print(f\"  {key}: {value_str}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8e8b8",
   "metadata": {},
   "source": [
    "### 1.3 Splitting Datasets\n",
    "\n",
    "Prepare train/test splits for evaluating agent performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eef3a7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: 4821 examples\n",
      "Test set: 961 examples\n"
     ]
    }
   ],
   "source": [
    "train_set = dataset['train']\n",
    "test_set = dataset['test']\n",
    "\n",
    "print(f\"\\nTrain set: {len(train_set)} examples\")\n",
    "print(f\"Test set: {len(test_set)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4724bd",
   "metadata": {},
   "source": [
    "## Part 2: Hugging Face Transformers\n",
    "\n",
    "### 2.1 Loading a Pre-trained Model\n",
    "\n",
    "Load a small LLM to show how the library works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f366f26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323d0604276e442a92bc0a1633d77cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/340M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1635f165b9614584a7b699da96f0539f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: MistralForCausalLM\n",
      "Number of parameters: 170,082,048\n",
      "Tokenizer vocabulary size: 32000\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"crumb/nano-mistral\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b3398f",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization - Converting Text to Model Input\n",
    "\n",
    "Transform prompts into tokens that the model understands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7196148f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "You are navigating a graph. Current position: node 2.\n",
      "Visited nodes: [1, 2].\n",
      "Available neighbors: [3, 4].\n",
      "Goal: reach node 5.\n",
      "Next move:\n",
      "\n",
      "Tokenized (first 20 tokens): [1, 995, 460, 27555, 1077, 264, 5246, 28723, 10929, 2840, 28747, 3179, 28705, 28750, 28723, 13, 5198, 1345, 9249, 28747]\n",
      "Total tokens: 49\n"
     ]
    }
   ],
   "source": [
    "# Example prompt for Proposer agent\n",
    "prompt = \"\"\"You are navigating a graph. Current position: node 2.\n",
    "Visited nodes: [1, 2].\n",
    "Available neighbors: [3, 4].\n",
    "Goal: reach node 5.\n",
    "Next move:\"\"\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Input text:\")\n",
    "print(prompt)\n",
    "print(f\"\\nTokenized (first 20 tokens): {inputs['input_ids'][0][:20].tolist()}\")\n",
    "print(f\"Total tokens: {len(inputs['input_ids'][0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1cff2b",
   "metadata": {},
   "source": [
    "### 2.3 Generating Agent Responses\n",
    "\n",
    "Use the model to generate text - this is how agents produce their suggestions and validations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa6cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "In a graph with nodes 1,2,3,4,5 and edges (1,2),(2,3),(3,4),(4,5), the shortest path from 1 to 5 is:\n",
      "\n",
      "Generated response:\n",
      "\n",
      "\n",
      "  (1,2) \n",
      "    (3,4)    (3,4)    (4,4)    (4,5)    (4,5)    (4,5)    (4,\n",
      "\n",
      "This is just to show how it works, to get a meaningful response, a larger model is needed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Generate response from Proposer agent\n",
    "prompt = \"In a graph with nodes 1,2,3,4,5 and edges (1,2),(2,3),(3,4),(4,5), the shortest path from 1 to 5 is:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate with specific parameters\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_new_tokens=50,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "print(\"\\nGenerated response:\")\n",
    "print(generated_text[len(prompt):])\n",
    "print(\"\\n(This is just to show how it works, to get a meaningful response, a larger model is needed.)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
